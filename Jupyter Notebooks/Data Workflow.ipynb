{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Workflow - Working with Biological Specimen Data Using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to provide an example of a data analysis workflow with biological specimen data using a collection of Python scripts written by the author. The purpose of these scripts is to make it quick and convenient for a user to specify a subset of data they have an interest in, retrieve all data that matches these conditions from a source on the internet, store this data in a locally hosted database and then perform various actions, such as queries or analysis, on the data in the local database through an API incorporated into the scripts. \n",
    "\n",
    "It is important to note that these scripts were written by the author as exploratory projects for learning how to program API's, how to programatically interact with API's, how to programatically interact with databases and how to use Python packages to analyze data. The resulting scripts from these learning processes are now being incorporated into a single workflow in this notebook to see how they interact together.\n",
    "\n",
    "This notebook will also provide some implementation details on the various modules within the collection of Python scripts that will be used in this notebook. This will be mainly for documentation & usage purposes and do not need to be read to know how to use the scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting & Storing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage in this workflow will consist of first specifying the source of biological specimen data to be used and then defining a subset of the data available there that is of some interst for later analysis purposes. Next, a query needs to be formatted to retreive this subset of interest from the data source's records. Finally, the results must be stored in a local database for later use.\n",
    "\n",
    "#### Data Source\n",
    "The source of biological specimen data used in this workflow will be the data available at Idigbio (Integrated Digitized Biocollections https://www.idigbio.org/), a site which has specimen records from various specimen collections across the U.S. The records in these collections in turn consist of a large variety of specimens from all over the world. Idigbio provides free access to these records not only through a web portal, but also through an API for more programmatic data retrieval needs. There also exist libraries for the API that make it easily accessible using programming languages like R, Python and Ruby. As an important side note, the Python scripts that this workflow incorporates rely on the Python specific package for this API called simply 'idigbio', which can be installed through pip or https://github.com/iDigBio/idigbio-search-api. \n",
    "\n",
    "#### Defining & querying for a subset of data\n",
    "Next, the subset of data to be explored needs to be defined so that its retrieval process can be started. A great way to explore possible data sets of interest is to use Idigbio's search portal and conduct a few prelimianry queries there to see what is the quality & quantity of records available. Once a suitable dataset is found, the search terms used to retrieve that dataset should be documented so that they can be used to programatically retrieve the same dataset using the author's Python scripts. This is, however, a completely voluntary step as the same queries can be conducted using the Python scripts as long as the correct search terms are used for the query, something which will be discussed in the next paragraph.\n",
    "\n",
    "To programatically retrieve the defined dataset, parameters must be given to the iDigBio API that match the search terms defined earlier, so that it can build a query and retrieve the appropriate records from Idigbio's database. In order to construct this query, the Idigbio API library in Python requires a dictionary (often named \"rq\" for record query) where the search terms are defined as key-value pairs. The keys in this dictionary must consist of the field name of interest in the record and the value of the corresponding value of interest. As the dictionary keys correspond to record field names, there are very specific terms that must be used for them, a full list of which can be found here: https://github.com/idigbio/idigbio-search-api/wiki/Index-Fields. These terms cover many variables commonly associated with biological specimen collection records like scientific name, family, genus among many others.\n",
    "\n",
    "For example, if the interest is in obtaining all records of collected specimen within the genus \"Panthera\" that are in the collection of the American Museum of Natural History (AMNH), the appropriate \"rq\" dictionary would look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq = {\"genus\" : \"panthera\", \"institutioncode\" : \"AMNH\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of May 2018, this query into Idigbio's records yielded about 668 results which is a suitable dataset size for this notebook. However, more search terms can be added to the dictionary to refine the query even further or terms can be eliminated to broaden it, yielding less or more records respectively.\n",
    "\n",
    "**PLEASE NOTE:** Due to restrictions of the idigbio API, the absolute maximum query result size that can be returned is 5000 records. Query result number returned can optionally be limited by specifying a variable called \"limit\" which is passed to the Idigbio API. To get the maximum amount of records, we will specify the limit as 5000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a local database\n",
    "Now that the parameters for the query to Idigbio have been defined, the next steps are to actually conduct the query and store the results. Thus, it is first necessary to set up a database to store the query results. In this notebook and the Python Scripts written for this workflow process, a PostgreSQL database has been selected for this task. As this is the case, the scripts use Python libraries like \"records\" and \"psycopg2\" which are meant for interfacing only with a PostgreSQL database. Consequently, in order to use the author's scripts a PostgreSQL database must be set up to which the scripts can connect to and make changes. The appropriate database connection details such as database name, user, password etc. **MUST** be set in the \"DBInfo.py\" script's \"connectDB()\" function to correspond to the user's database settings before executing any code below.\n",
    "\n",
    "#### Retrieving the data from iDigBio\n",
    "Once a PostgreSQL database has been set up and appropriate parameters set in the \"DBInfo.py\" script, the query for and storage of the target data can be done. The first step in this process is to define the name of the table in which the data will be stored in the PostgreSQL database, a table of the same name should not already exist in the database as the script will automatically create a new table. The table name can be defined as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"records\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using the author's Python script called \"API.py\", the query itself and storage of resulting data from the query can be done using just one call to the function \"createTable()\". This function can take 3 arguments, two of which are required and one which is optional. The first positional argument is the \"rq\" dictionary defined earlier (which contains the desired search terms), the second positional argument is the table_name string defined above and the third optional argument is the \"limit\" variable used for limiting the number of records retrieved from iDigbBio (5000 by default). The function can be used as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database table records has been created successfully.\n",
      "Database table records has been populated successfully.\n"
     ]
    }
   ],
   "source": [
    "import API\n",
    "\n",
    "API.createTable(rq, table_name, limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the function has finished executing, there should be a new table in the database specified containing all of the data returned from the query. As a sidenote on the structure of the author's scripts, \"API.py\" is the public interface to all of the other scripts meaning that the functions in this script provide access to all the functionalities implemented in the other scripts. The functions present in \"API.py\" will be discussed in greater detail throughout this notebook as they are used, starting from the createTable() function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the createTable() function works\n",
    "This subsection will briefly discuss how the createTable() function creates a table in the PostgreSQL database using the results given by the user's query. This section does not provide necessary knowledge for using the scripts and is more for documentation purposes.\n",
    "\n",
    "This function utilizes two helper scripts: \"TableSchemaCreator.py\" and \"TableCreator.py\". The former script's purpose is to create all the necessary fields into the database table and the latter script's purpose is to then populate those fields.\n",
    "\n",
    "The way \"TableSchemaCreator.py\" creates the table fields is dynamic. This means that it is passed the results from the query to iDigBio, which it then uses to create a list of distinct field names present in that data. Quite simply, the script iteratively looks at the field names in each record present in the query results, compares them to field names present in the database table and then adds the ones not already present there to the table. Additionally, the Idigbio API provides an endpoint from which the types of each field name can be gathered. By taking the list of field names gathered and the type of each field, a SQL command is formed for adding each field and its corresponding type to the table.\n",
    "\n",
    "The \"TableCreator.py\" script is also given the results of the query, but its main purpose is to populate the newly created table. As a table with every field present in the query has been formed by the previous script, this script simply goes through the query result record by record and creates INSERT statements that can then be executed in the database to input the data.\n",
    "\n",
    "The interaction between the database and the program is entirely done using the \"psycopg2\" library in Python, it can be installed using pip and it has further documentation here: http://initd.org/psycopg/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on the format of data returned from iDigBio & how it is mapped to the local database\n",
    "The records returned from iDigBio's database are primarily in JSON format, which is automatically converted to a Python dictionary by the iDigBio API. Each record is a dictionary containing field names as keys and their corresponding values as the dictionary values. These record dictionaries are also fairly flat, meaning there are not many fields with nested datastructures within them. The few fields that differ from this primarily contain arrays or dictionaries. In the making of the scripts for this workflow, it was decided to flatten these datastructures by simply storing them as strings of text rather than seperate tables. This was done so that the local database has a completely flat table structure, but still preserves all of the available information. This means that while most fields have fairly short values, there are a few fields with typically very long strings within them that need to be parsed before they are usable. Typical examples would be the \"indexData\" and \"flags\" fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieving the Data from the PostgreSQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been retrieved from iDigBio and stored in the local PostgreSQL database, the goal is to retrieve the data from this local database so it can be used for other purposes like analysis. This will be done in a similar way that data was retrieved from iDigBio, meaning that it will be accessed through an API. As this time the local database is being queried and clearly iDigBio's API can not be used for retrieving data from there, a seperate API has been built for this purpose. This custom API has been built to provide very similar functionalities to the iDigBio API for consistency. The following paragraphs will further discuss how to use this custom API and how the data retrieval process will work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launching the API server\n",
    "Before the API for retrieving data from the local database can be used, the server that processes the requests from the API must be first launched. The script for launching the server, and which contains all the necessary URL routes & responses that the server operates by, is called \"APIServer.py\". This script contains a fairly simple web-framework implementation done using the Python library called \"Bottle\" (https://bottlepy.org/docs/dev/). The web address & port of the server are defined in this script and thus can be modified there if necessary.\n",
    "\n",
    "**PLEASE NOTE** Since the scripts retrieving the data from the database send web-requests to the API's server, both these scripts and the server must be run concurrently on the local computer. This means that the \"APIServer.py\" script should be run in its own terminal/kernel. Once the \"APIServer.py\" script has been launched, it is ready to process requests sent to it using the other scripts. It must be emphasized again that in order for the functions retrieving data from the local database to work, the server must be running when they are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the API to retrieve multiple records from the local database\n",
    "As mentioned previously, the API provided with the author's scripts functions very similarly to the API provided by iDigBio. Most importantly, this means that it uses the same method to accept query search terms from the user in the form of the \"rq\" dictionary, where the terms are provided by the user as key-value pairs. In addition, it also accepts the limit argument for limiting the no. of records returned from the query to the database.\n",
    "\n",
    "At this point, the local database contains the subset of data from iDigBio that was defined earlier consisting of 668 records of specimen in the records of the American Museum of Natural History that are in the genus Panthera. To retrieve all of these records from the database, the API could simply be given the same \"rq\" dictionary that was given to iDigBio. A better example would be, however, to choose a subset of this data like the first 5 specimens that are tigers, or in other words have the scientific name \"Panthera Tigris\". This can be achieved with the following \"rq\" dictionary and limit variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq = {\"scientificname\":\"Panthera Tigris\"}\n",
    "limit = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the search terms in this dictionary must be present in the list for allowed search terms discussed earlier.\n",
    "\n",
    "As the local database is being queried, the name of the table where the data has been stored must also be initialized as a variable. As this was already done above in this notebook, this step will be skipped in this example. However, this is an important thing to note in the case that a table that has been made previously is being queried.\n",
    "\n",
    "The actual query will be done using the \"API.py\" script's function called \"searchRecords()\" which takes three arguments, two of which are required (rq, table_name) and the third (limit) optional. The query can be performed as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = API.searchRecords(rq, table_name, limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from this query will be returned by the \"searchRecords()\" function in a Python dictionary of the following (simplified) format:\n",
    "\n",
    "    results = {\n",
    "                \"itemCount\" : 5\n",
    "                \"items\" : [\n",
    "                            {record1},\n",
    "                            {record2},\n",
    "                            ...\n",
    "                            {record5}\n",
    "                          ]\n",
    "              }\n",
    "\n",
    "Where each \"recordX\" entry in the \"items\" array is a record from the database in dictionary format. These keys in each record dictionary directly correspond to the field names in the database. The \"itemCount\" variable simply tracks the no. of records that are contained in the result dictionary. This format mirrors, to some extent, the format in which records are returned by the iDigBio API which was discussed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing the query results\n",
    "Example 1. Viewing the no. of records found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(results[\"itemCount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2. Viewing a single record in the query results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'basisofrecord': 'preservedspecimen',\n",
      " 'canonicalname': 'panthera tigris',\n",
      " 'catalognumber': 'm-14030',\n",
      " 'class': 'mammalia',\n",
      " 'collectioncode': 'mammals',\n",
      " 'collector': 'r. weber',\n",
      " 'continent': 'asia',\n",
      " 'country': 'indonesia',\n",
      " 'countrycode': 'idn',\n",
      " 'county': None,\n",
      " 'datasetid': '7ddf754f-d193-4cc9-b351-99906754a03b',\n",
      " 'datecollected': '1896-01-18',\n",
      " 'datemodified': '2017-01-08',\n",
      " 'dqs': '0.30434782608695654',\n",
      " 'etag': '12dfeb65103d1a1fcf5e710e3c6bb4e6e3f2709d',\n",
      " 'eventdate': '1896-01-18',\n",
      " 'family': 'felidae',\n",
      " 'genus': 'panthera',\n",
      " 'hasImage': False,\n",
      " 'hasMedia': False,\n",
      " 'highertaxon': 'animalia;chordata;mammalia;carnivora;felidae;panthera',\n",
      " 'individualcount': '1',\n",
      " 'infraspecificepithet': None,\n",
      " 'institutioncode': 'amnh',\n",
      " 'institutionid': 'urn:lsid:biocol.org:col:34925',\n",
      " 'kingdom': 'animalia',\n",
      " 'locality': None,\n",
      " 'maxelevation': None,\n",
      " 'minelevation': None,\n",
      " 'municipality': None,\n",
      " 'occurrenceid': 'urn:catalog:amnh:mammals:m-14030',\n",
      " 'order': 'carnivora',\n",
      " 'phylum': 'chordata',\n",
      " 'recordids': '[\"cb790bee-26da-40ed-94e0-d179618f9bd4\\\\\\\\urn:catalog:amnh:mammals:m-14030\"]',\n",
      " 'recordnumber': '622',\n",
      " 'recordset': 'cb790bee-26da-40ed-94e0-d179618f9bd4',\n",
      " 'scientificname': 'panthera tigris',\n",
      " 'specificepithet': 'tigris',\n",
      " 'startdayofyear': 18,\n",
      " 'stateprovince': None,\n",
      " 'taxonid': '5219416',\n",
      " 'taxonomicstatus': 'accepted',\n",
      " 'taxonrank': 'species',\n",
      " 'typestatus': None,\n",
      " 'uuid': 'efc07c69-6e42-48ca-9930-0960bb5f1823',\n",
      " 'verbatimeventdate': '1896-01-18',\n",
      " 'verbatimlocality': 'eurasia; indonesia; sumatra; ; ; ; ; ; ; ;',\n",
      " 'waterbody': 'indian ocean'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pformat\n",
    "\n",
    "# Deleting keys with large strings for clarity\n",
    "del results[\"items\"][0][\"indexData\"]\n",
    "del results[\"items\"][0][\"flags\"]\n",
    "\n",
    "# Printing out record\n",
    "print(pformat(results[\"items\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
